{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653cdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.0.tar.gz (434.1 MB)\n",
      "     ---------------------------------------- 0.0/434.1 MB ? eta -:--:--\n",
      "     --------------------------------------- 2.6/434.1 MB 13.1 MB/s eta 0:00:33\n",
      "     --------------------------------------- 5.2/434.1 MB 12.6 MB/s eta 0:00:34\n",
      "      -------------------------------------- 7.9/434.1 MB 12.5 MB/s eta 0:00:35\n",
      "      ------------------------------------- 10.7/434.1 MB 12.6 MB/s eta 0:00:34\n",
      "     - ------------------------------------ 13.4/434.1 MB 12.5 MB/s eta 0:00:34\n",
      "     - ------------------------------------ 16.0/434.1 MB 12.5 MB/s eta 0:00:34\n",
      "     - ------------------------------------ 18.4/434.1 MB 12.5 MB/s eta 0:00:34\n",
      "     - ------------------------------------ 21.0/434.1 MB 12.4 MB/s eta 0:00:34\n",
      "     -- ----------------------------------- 23.9/434.1 MB 12.5 MB/s eta 0:00:33\n",
      "     -- ----------------------------------- 26.2/434.1 MB 12.4 MB/s eta 0:00:33\n",
      "     -- ----------------------------------- 28.8/434.1 MB 12.5 MB/s eta 0:00:33\n",
      "     -- ----------------------------------- 31.5/434.1 MB 12.4 MB/s eta 0:00:33\n",
      "     -- ----------------------------------- 34.1/434.1 MB 12.4 MB/s eta 0:00:33\n",
      "     --- ---------------------------------- 36.4/434.1 MB 12.4 MB/s eta 0:00:32\n",
      "     --- ---------------------------------- 39.1/434.1 MB 12.4 MB/s eta 0:00:32\n",
      "     --- ---------------------------------- 41.7/434.1 MB 12.4 MB/s eta 0:00:32\n",
      "     --- ---------------------------------- 44.6/434.1 MB 12.4 MB/s eta 0:00:32\n",
      "     ---- --------------------------------- 47.2/434.1 MB 12.4 MB/s eta 0:00:32\n",
      "     ---- --------------------------------- 49.8/434.1 MB 12.4 MB/s eta 0:00:31\n",
      "     ---- --------------------------------- 50.9/434.1 MB 12.4 MB/s eta 0:00:31\n",
      "     ---- --------------------------------- 54.8/434.1 MB 12.4 MB/s eta 0:00:31\n",
      "     ----- -------------------------------- 57.4/434.1 MB 12.4 MB/s eta 0:00:31\n",
      "     ----- -------------------------------- 60.3/434.1 MB 12.4 MB/s eta 0:00:31\n",
      "     ----- -------------------------------- 62.9/434.1 MB 12.4 MB/s eta 0:00:30\n",
      "     ----- -------------------------------- 65.5/434.1 MB 12.4 MB/s eta 0:00:30\n",
      "     ----- -------------------------------- 68.2/434.1 MB 12.4 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 71.0/434.1 MB 12.4 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 73.7/434.1 MB 12.4 MB/s eta 0:00:30\n",
      "     ------ ------------------------------- 76.3/434.1 MB 12.4 MB/s eta 0:00:29\n",
      "     ------ ------------------------------- 79.2/434.1 MB 12.4 MB/s eta 0:00:29\n",
      "     ------- ------------------------------ 81.8/434.1 MB 12.4 MB/s eta 0:00:29\n",
      "     ------- ------------------------------ 84.1/434.1 MB 12.4 MB/s eta 0:00:29\n",
      "     ------- ------------------------------ 86.8/434.1 MB 12.4 MB/s eta 0:00:28\n",
      "     ------- ------------------------------ 89.4/434.1 MB 12.4 MB/s eta 0:00:28\n",
      "     -------- ----------------------------- 92.0/434.1 MB 12.4 MB/s eta 0:00:28\n",
      "     -------- ----------------------------- 94.9/434.1 MB 12.4 MB/s eta 0:00:28\n",
      "     -------- ----------------------------- 97.5/434.1 MB 12.4 MB/s eta 0:00:28\n",
      "     -------- ---------------------------- 100.1/434.1 MB 12.4 MB/s eta 0:00:27\n",
      "     -------- ---------------------------- 102.8/434.1 MB 12.4 MB/s eta 0:00:27\n",
      "     -------- ---------------------------- 105.4/434.1 MB 12.4 MB/s eta 0:00:27\n",
      "     --------- --------------------------- 108.0/434.1 MB 12.4 MB/s eta 0:00:27\n",
      "     --------- --------------------------- 110.9/434.1 MB 12.4 MB/s eta 0:00:27\n",
      "     --------- --------------------------- 113.5/434.1 MB 12.4 MB/s eta 0:00:26\n",
      "     --------- --------------------------- 116.1/434.1 MB 12.4 MB/s eta 0:00:26\n",
      "     ---------- -------------------------- 118.8/434.1 MB 12.4 MB/s eta 0:00:26\n",
      "     ---------- -------------------------- 121.4/434.1 MB 12.4 MB/s eta 0:00:26\n",
      "     ---------- -------------------------- 124.3/434.1 MB 12.4 MB/s eta 0:00:25\n",
      "     ---------- -------------------------- 126.9/434.1 MB 12.4 MB/s eta 0:00:25\n",
      "     ----------- ------------------------- 129.5/434.1 MB 12.4 MB/s eta 0:00:25\n",
      "     ----------- ------------------------- 132.1/434.1 MB 12.4 MB/s eta 0:00:25\n",
      "     ----------- ------------------------- 134.7/434.1 MB 12.4 MB/s eta 0:00:25\n",
      "     ----------- ------------------------- 137.4/434.1 MB 12.4 MB/s eta 0:00:24\n",
      "     ----------- ------------------------- 139.7/434.1 MB 12.4 MB/s eta 0:00:24\n",
      "     ------------ ------------------------ 142.3/434.1 MB 12.4 MB/s eta 0:00:24\n",
      "     ------------ ------------------------ 145.2/434.1 MB 12.4 MB/s eta 0:00:24\n",
      "     ------------ ------------------------ 147.8/434.1 MB 12.4 MB/s eta 0:00:24\n",
      "     ------------ ------------------------ 150.5/434.1 MB 12.4 MB/s eta 0:00:23\n",
      "     ------------- ----------------------- 153.1/434.1 MB 12.4 MB/s eta 0:00:23\n",
      "     ------------- ----------------------- 155.5/434.1 MB 12.4 MB/s eta 0:00:23\n",
      "     ------------- ----------------------- 158.1/434.1 MB 12.4 MB/s eta 0:00:23\n",
      "     ------------- ----------------------- 160.4/434.1 MB 12.4 MB/s eta 0:00:23\n",
      "     ------------- ----------------------- 163.1/434.1 MB 12.4 MB/s eta 0:00:22\n",
      "     -------------- ---------------------- 165.7/434.1 MB 12.4 MB/s eta 0:00:22\n",
      "     -------------- ---------------------- 168.3/434.1 MB 12.4 MB/s eta 0:00:22\n",
      "     -------------- ---------------------- 170.9/434.1 MB 12.4 MB/s eta 0:00:22\n",
      "     -------------- ---------------------- 173.5/434.1 MB 12.4 MB/s eta 0:00:22\n",
      "     --------------- --------------------- 176.2/434.1 MB 12.4 MB/s eta 0:00:21\n",
      "     --------------- --------------------- 179.3/434.1 MB 12.4 MB/s eta 0:00:21\n",
      "     --------------- --------------------- 181.7/434.1 MB 12.4 MB/s eta 0:00:21\n",
      "     --------------- --------------------- 184.5/434.1 MB 12.4 MB/s eta 0:00:21\n",
      "     --------------- --------------------- 186.9/434.1 MB 12.4 MB/s eta 0:00:20\n",
      "     ---------------- -------------------- 189.5/434.1 MB 12.4 MB/s eta 0:00:20\n",
      "     ---------------- -------------------- 192.2/434.1 MB 12.4 MB/s eta 0:00:20\n",
      "     ---------------- -------------------- 194.5/434.1 MB 12.4 MB/s eta 0:00:20\n",
      "     ---------------- -------------------- 197.1/434.1 MB 12.4 MB/s eta 0:00:20\n",
      "     ----------------- ------------------- 199.8/434.1 MB 12.4 MB/s eta 0:00:19\n",
      "     ----------------- ------------------- 202.4/434.1 MB 12.4 MB/s eta 0:00:19\n",
      "     ----------------- ------------------- 204.7/434.1 MB 12.4 MB/s eta 0:00:19\n",
      "     ----------------- ------------------- 207.4/434.1 MB 12.4 MB/s eta 0:00:19\n",
      "     ----------------- ------------------- 210.2/434.1 MB 12.4 MB/s eta 0:00:19\n",
      "     ------------------ ------------------ 212.3/434.1 MB 12.4 MB/s eta 0:00:18\n",
      "     ------------------ ------------------ 215.2/434.1 MB 12.4 MB/s eta 0:00:18\n",
      "     ------------------ ------------------ 217.8/434.1 MB 12.4 MB/s eta 0:00:18\n",
      "     ------------------ ------------------ 220.5/434.1 MB 12.4 MB/s eta 0:00:18\n",
      "     ------------------- ----------------- 223.3/434.1 MB 12.4 MB/s eta 0:00:18\n",
      "     ------------------- ----------------- 226.0/434.1 MB 12.4 MB/s eta 0:00:17\n",
      "     ------------------- ----------------- 228.3/434.1 MB 12.4 MB/s eta 0:00:17\n",
      "     ------------------- ----------------- 230.9/434.1 MB 12.4 MB/s eta 0:00:17\n",
      "     ------------------- ----------------- 233.3/434.1 MB 12.4 MB/s eta 0:00:17\n",
      "     -------------------- ---------------- 235.9/434.1 MB 12.4 MB/s eta 0:00:17\n",
      "     -------------------- ---------------- 238.6/434.1 MB 12.4 MB/s eta 0:00:16\n",
      "     -------------------- ---------------- 241.2/434.1 MB 12.4 MB/s eta 0:00:16\n",
      "     -------------------- ---------------- 243.5/434.1 MB 12.4 MB/s eta 0:00:16\n",
      "     -------------------- ---------------- 246.2/434.1 MB 12.4 MB/s eta 0:00:16\n",
      "     --------------------- --------------- 249.0/434.1 MB 12.4 MB/s eta 0:00:15\n",
      "     --------------------- --------------- 251.1/434.1 MB 12.4 MB/s eta 0:00:15\n",
      "     --------------------- --------------- 254.0/434.1 MB 12.4 MB/s eta 0:00:15\n",
      "     --------------------- --------------- 256.6/434.1 MB 12.4 MB/s eta 0:00:15\n",
      "     ---------------------- -------------- 259.3/434.1 MB 12.4 MB/s eta 0:00:15\n",
      "     ---------------------- -------------- 261.9/434.1 MB 12.4 MB/s eta 0:00:14\n",
      "     ---------------------- -------------- 264.5/434.1 MB 12.4 MB/s eta 0:00:14\n",
      "     ---------------------- -------------- 267.1/434.1 MB 12.4 MB/s eta 0:00:14\n",
      "     ---------------------- -------------- 269.7/434.1 MB 12.4 MB/s eta 0:00:14\n",
      "     ----------------------- ------------- 272.1/434.1 MB 12.4 MB/s eta 0:00:14\n",
      "     ----------------------- ------------- 274.5/434.1 MB 12.4 MB/s eta 0:00:13\n",
      "     ----------------------- ------------- 277.1/434.1 MB 12.4 MB/s eta 0:00:13\n",
      "     ----------------------- ------------- 279.7/434.1 MB 12.4 MB/s eta 0:00:13\n",
      "     ------------------------ ------------ 282.3/434.1 MB 12.4 MB/s eta 0:00:13\n",
      "     ------------------------ ------------ 285.0/434.1 MB 12.4 MB/s eta 0:00:13\n",
      "     ------------------------ ------------ 287.3/434.1 MB 12.4 MB/s eta 0:00:12\n",
      "     ------------------------ ------------ 289.9/434.1 MB 12.4 MB/s eta 0:00:12\n",
      "     ------------------------ ------------ 292.8/434.1 MB 12.4 MB/s eta 0:00:12\n",
      "     ------------------------- ----------- 295.4/434.1 MB 12.4 MB/s eta 0:00:12\n",
      "     ------------------------- ----------- 298.1/434.1 MB 12.4 MB/s eta 0:00:11\n",
      "     ------------------------- ----------- 300.9/434.1 MB 12.4 MB/s eta 0:00:11\n",
      "     ------------------------- ----------- 303.3/434.1 MB 12.4 MB/s eta 0:00:11\n",
      "     -------------------------- ---------- 305.9/434.1 MB 12.4 MB/s eta 0:00:11\n",
      "     -------------------------- ---------- 308.5/434.1 MB 12.4 MB/s eta 0:00:11\n",
      "     -------------------------- ---------- 310.9/434.1 MB 12.4 MB/s eta 0:00:10\n",
      "     -------------------------- ---------- 313.5/434.1 MB 12.5 MB/s eta 0:00:10\n",
      "     -------------------------- ---------- 315.9/434.1 MB 12.4 MB/s eta 0:00:10\n",
      "     --------------------------- --------- 318.5/434.1 MB 12.4 MB/s eta 0:00:10\n",
      "     --------------------------- --------- 321.1/434.1 MB 12.4 MB/s eta 0:00:10\n",
      "     --------------------------- --------- 323.5/434.1 MB 12.4 MB/s eta 0:00:09\n",
      "     --------------------------- --------- 326.1/434.1 MB 12.4 MB/s eta 0:00:09\n",
      "     ---------------------------- -------- 328.7/434.1 MB 12.4 MB/s eta 0:00:09\n",
      "     ---------------------------- -------- 331.6/434.1 MB 12.4 MB/s eta 0:00:09\n",
      "     ---------------------------- -------- 334.2/434.1 MB 12.4 MB/s eta 0:00:09\n",
      "     ---------------------------- -------- 336.6/434.1 MB 12.4 MB/s eta 0:00:08\n",
      "     ---------------------------- -------- 339.2/434.1 MB 12.4 MB/s eta 0:00:08\n",
      "     ----------------------------- ------- 341.8/434.1 MB 12.4 MB/s eta 0:00:08\n",
      "     ----------------------------- ------- 344.5/434.1 MB 12.4 MB/s eta 0:00:08\n",
      "     ----------------------------- ------- 347.1/434.1 MB 12.4 MB/s eta 0:00:08\n",
      "     ----------------------------- ------- 349.4/434.1 MB 12.4 MB/s eta 0:00:07\n",
      "     ------------------------------ ------ 352.3/434.1 MB 12.4 MB/s eta 0:00:07\n",
      "     ------------------------------ ------ 354.9/434.1 MB 12.4 MB/s eta 0:00:07\n",
      "     ------------------------------ ------ 357.3/434.1 MB 12.4 MB/s eta 0:00:07\n",
      "     ------------------------------ ------ 359.9/434.1 MB 12.4 MB/s eta 0:00:06\n",
      "     ------------------------------ ------ 362.5/434.1 MB 12.4 MB/s eta 0:00:06\n",
      "     ------------------------------- ----- 365.2/434.1 MB 12.4 MB/s eta 0:00:06\n",
      "     ------------------------------- ----- 368.1/434.1 MB 12.4 MB/s eta 0:00:06\n",
      "     ------------------------------- ----- 370.4/434.1 MB 12.4 MB/s eta 0:00:06\n",
      "     ------------------------------- ----- 373.0/434.1 MB 12.4 MB/s eta 0:00:05\n",
      "     -------------------------------- ---- 375.7/434.1 MB 12.4 MB/s eta 0:00:05\n",
      "     -------------------------------- ---- 378.3/434.1 MB 12.4 MB/s eta 0:00:05\n",
      "     -------------------------------- ---- 380.6/434.1 MB 12.4 MB/s eta 0:00:05\n",
      "     -------------------------------- ---- 383.3/434.1 MB 12.4 MB/s eta 0:00:05\n",
      "     -------------------------------- ---- 385.9/434.1 MB 12.4 MB/s eta 0:00:04\n",
      "     --------------------------------- --- 388.5/434.1 MB 12.4 MB/s eta 0:00:04\n",
      "     --------------------------------- --- 391.1/434.1 MB 12.4 MB/s eta 0:00:04\n",
      "     --------------------------------- --- 393.5/434.1 MB 12.4 MB/s eta 0:00:04\n",
      "     --------------------------------- --- 396.1/434.1 MB 12.4 MB/s eta 0:00:04\n",
      "     --------------------------------- --- 398.7/434.1 MB 12.4 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 401.3/434.1 MB 12.4 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 403.7/434.1 MB 12.4 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 406.3/434.1 MB 12.4 MB/s eta 0:00:03\n",
      "     ---------------------------------- -- 408.9/434.1 MB 12.4 MB/s eta 0:00:03\n",
      "     ----------------------------------- - 411.3/434.1 MB 12.4 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 414.2/434.1 MB 12.4 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 416.8/434.1 MB 12.4 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 419.4/434.1 MB 12.4 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 422.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  424.9/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  427.6/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  430.2/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  432.8/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------  434.1/434.1 MB 12.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 434.1/434.1 MB 8.4 MB/s  0:00:44\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): still running...\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-4.0.0-py2.py3-none-any.whl size=434741340 sha256=525ab2259e7c2dd0db7839a1bcdd9c8844cf1a3b2a36cce63d7c9dde9e36feed\n",
      "  Stored in directory: c:\\users\\charson\\appdata\\local\\pip\\cache\\wheels\\e4\\82\\ed\\8c205a7ade6132d277fcdaccff39051342fff763b34e90dc8f\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   ---------------------------------------- 2/2 [pyspark]\n",
      "\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.0\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78cf58d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o48.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:42)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:92)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\r\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:189)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.jdbc(DataFrameReader.scala:115)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\r\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\r\n\t\tat scala.Option.foreach(Option.scala:437)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:42)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 23 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     10\u001b[39m connection_properties = {\n\u001b[32m     11\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mpostgres\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mpassword\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33mAdmin1234$\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mdriver\u001b[39m\u001b[33m\"\u001b[39m : \u001b[33m\"\u001b[39m\u001b[33morg.postgresql.Driver\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m }\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# select * from persons\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m df_all_persons = \u001b[43mspark_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpersons\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_properties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_all_persons.head())\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# select * from persons where age > 19\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Charson\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1181\u001b[39m, in \u001b[36mDataFrameReader.jdbc\u001b[39m\u001b[34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[39m\n\u001b[32m   1177\u001b[39m     jpredicates = utils.to_java_array(\n\u001b[32m   1178\u001b[39m         gateway, \u001b[38;5;28mgetattr\u001b[39m(gateway.jvm, \u001b[33m\"\u001b[39m\u001b[33mjava.lang.String\u001b[39m\u001b[33m\"\u001b[39m), predicates\n\u001b[32m   1179\u001b[39m     )\n\u001b[32m   1180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28mself\u001b[39m._jreader.jdbc(url, table, jpredicates, jprop))\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Charson\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Charson\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Charson\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o48.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:42)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\r\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:92)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\r\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:189)\r\n\tat org.apache.spark.sql.classic.DataFrameReader.jdbc(DataFrameReader.scala:115)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\r\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\r\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\r\n\t\tat scala.Option.foreach(Option.scala:437)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:112)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:42)\r\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:361)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\r\n\t\tat scala.Option.getOrElse(Option.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\r\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\r\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\r\n\t\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\r\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\r\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\r\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 23 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "  .appName(\"PySpark JDBC Example\") \\\n",
    "  .config(\"spark.jars\", \"d/github/bootcamp-python/postgresql-42.7.7.jar\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "jdbc_url=\"jdbc:postgresql://localhost:5432/bc2504p\"\n",
    "\n",
    "connection_properties = {\n",
    "  \"user\" : \"postgres\",\n",
    "  \"password\" : \"Admin1234$\",\n",
    "  \"driver\" : \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# select * from persons\n",
    "df_all_persons = spark_session.read.jdbc(url=jdbc_url, table=\"persons\", properties=connection_properties)\n",
    "print(df_all_persons.show())\n",
    "\n",
    "# select * from persons where age > 19\n",
    "query_sql_result = \"(select * from persons where age > 19) as result\"\n",
    "df_age_above_19_persons = spark_session.read.jdbc(url=jdbc_url, table=query_sql_result, properties=connection_properties)\n",
    "\n",
    "print(df_age_above_19_persons.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
